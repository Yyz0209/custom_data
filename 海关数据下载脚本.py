import os
import time
import json
import re
from datetime import datetime

from playwright.sync_api import sync_playwright

from config import (
    BASE_URL,
    RAW_DATA_PATH,
    USER_AGENT,
    HEADLESS,
    NAV_TIMEOUT_MS,
    SELECTOR_TIMEOUT_MS,
    NAV_MAX_RETRIES,
    NAV_RETRY_DELAY_SEC,
    SLOW_MO_MS,
)
import random
import pandas as pd
from data_utils import ensure_raw_data_dir, month_file_name, parse_table_html_to_df, save_raw_df


def scrape_raw_data_to_csv() -> None:
    """
    ä¸‹è½½è‡ª 2024 å¹´èµ·æ‰€æœ‰å¯è§æœˆä»½çš„åŸå§‹è¡¨æ ¼ï¼Œå¹¶åŸæ ·ä¿å­˜ä¸º CSV åˆ° `raw_csv_data/`ã€‚
    """
    ensure_raw_data_dir()
    print(f"åŸå§‹æ•°æ®å°†ä¿å­˜åœ¨ '{RAW_DATA_PATH}' æ–‡ä»¶å¤¹ä¸­ã€‚")

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=HEADLESS, 
            slow_mo=SLOW_MO_MS,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-blink-features=AutomationControlled'
            ]
        )
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080},
            locale='zh-CN'
        )
        page = context.new_page()

        try:
            current_year = datetime.now().year
            for year in range(2024, current_year + 1):
                try:
                    print(f"å¯¼èˆªåˆ°ä¸»é¡µé¢ï¼Œå‡†å¤‡å¤„ç†å¹´ä»½: {year}")
                    # å¯¼èˆªåŠ å…¥é‡è¯•ï¼Œé€‚é…æ…¢ç½‘/ä»£ç†
                    for attempt in range(1, NAV_MAX_RETRIES + 1):
                        try:
                            page.goto(BASE_URL, timeout=NAV_TIMEOUT_MS)
                            page.wait_for_load_state("domcontentloaded")
                            page.wait_for_selector("//div[@class='customs-foot']", timeout=SELECTOR_TIMEOUT_MS)
                            break
                        except Exception as nav_err:
                            if attempt == NAV_MAX_RETRIES:
                                raise nav_err
                            print(f"    å¯¼èˆªå¤±è´¥ï¼Œé‡è¯• {attempt}/{NAV_MAX_RETRIES - 1} ...")
                            time.sleep(NAV_RETRY_DELAY_SEC)
                    print("    ä¸»é¡µé¢åŠ è½½æˆåŠŸï¼")

                    year_button_selector = f"//a[contains(text(), '{year}')]"
                    page.wait_for_selector(year_button_selector, timeout=20000).click()
                    time.sleep(3)

                    table_row_selector = "//tr[contains(., 'è¿›å‡ºå£å•†å“æ”¶å‘è´§äººæ‰€åœ¨åœ°æ€»å€¼è¡¨')]"
                    row = page.wait_for_selector(table_row_selector, timeout=20000)

                    month_texts = [
                        link.inner_text()
                        for link in row.query_selector_all("a")
                        if link.get_attribute("href")
                    ]
                    print(f"    æ‰¾åˆ° {year} å¹´å¯ç”¨çš„æœˆä»½: {month_texts}")

                    for month_text in month_texts:
                        if "æœˆ" not in month_text:
                            continue
                        if not month_text.replace("æœˆ", "").strip().isdigit():
                            continue

                        month_num = int(month_text.replace("æœˆ", "").strip())
                        
                        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™è·³è¿‡
                        file_path = os.path.join(RAW_DATA_PATH, month_file_name(year, month_num))
                        if os.path.exists(file_path):
                            print(f"  âœ… {year}å¹´{month_num}æœˆå…¨å›½æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
                            continue
                        
                        print(f"  ğŸ“¥ ä¸‹è½½ {year}å¹´{month_num}æœˆ å…¨å›½æ•°æ®...")

                        month_link_selector = (
                            "//tr[contains(., 'è¿›å‡ºå£å•†å“æ”¶å‘è´§äººæ‰€åœ¨åœ°æ€»å€¼è¡¨')]//a[text()='{}']".format(
                                month_text
                            )
                        )
                        month_link_element = page.wait_for_selector(month_link_selector, timeout=SELECTOR_TIMEOUT_MS)

                        # ç‚¹å‡»æ‰“å¼€æ–°é¡µç­¾ï¼›ä¸ªåˆ«æƒ…å†µä¸‹ç›®æ ‡ä¼šåœ¨åŒé¡µæ‰“å¼€ï¼Œå› æ­¤åšä¸¤ç§åˆ†æ”¯å¤„ç†
                        with context.expect_page() as new_page_info:
                            month_link_element.click()
                        detail_page = new_page_info.value if new_page_info.value else page
                        detail_page.wait_for_load_state("domcontentloaded", timeout=NAV_TIMEOUT_MS)

                        table_container_selector = "div.easysite-news-text"
                        try:
                            detail_page.wait_for_selector(table_container_selector, timeout=SELECTOR_TIMEOUT_MS)
                        except Exception:
                            # æœ‰æ—¶å€™æ–°é—»æ­£æ–‡å®¹å™¨ class å˜åŒ–æˆ–è¢«åŒ…è£¹ï¼Œé€€è€Œæ±‚å…¶æ¬¡ç›´æ¥æŠ“å– body
                            table_container_selector = "body"
                            detail_page.wait_for_selector(table_container_selector, timeout=SELECTOR_TIMEOUT_MS)
                        table_html = detail_page.locator(table_container_selector).inner_html()

                        # è§£æè¡¨æ ¼ï¼šä¼˜å…ˆå¤šçº§è¡¨å¤´ï¼›è‹¥å¤±è´¥ï¼Œé™çº§ header=0
                        try:
                            df = parse_table_html_to_df(table_html)
                        except Exception:
                            import pandas as pd
                            df_list = pd.read_html(table_html, header=0)
                            if not df_list:
                                raise ValueError("æœªè§£æåˆ°è¡¨æ ¼")
                            df = df_list[0]
                        file_path = os.path.join(RAW_DATA_PATH, month_file_name(year, month_num))
                        save_raw_df(df, file_path)
                        print(f"    åŸå§‹æ•°æ®å·²ä¿å­˜è‡³: {file_path}")

                        try:
                            detail_page.close()
                        except Exception:
                            pass
                        # éšæœºæŠ–åŠ¨ï¼Œé™ä½è§¦å‘é¢‘ç‡
                        time.sleep(random.uniform(2.0, 5.0))

                except Exception as exc:
                    print(f"å¤„ç†å¹´ä»½ {year} æ—¶å‡ºé”™: {exc}")
                    continue

            print("\næ‰€æœ‰åŸå§‹æ•°æ®çˆ¬å–å®Œæˆï¼")
        finally:
            print("ä»»åŠ¡ç»“æŸï¼Œæ­£åœ¨å…³é—­æµè§ˆå™¨...")
            browser.close()


def download_zhejiang_data():
    """åŠ¨æ€è¯†åˆ«æ­å·æµ·å…³â€˜ç»Ÿè®¡æ•°æ®â€™æ ç›®ä¸‹æœ€æ–°å¹´ä»½/æœˆä»½ï¼Œä¸‹è½½æµ™æ±Ÿçœæ•°æ®ä¸ºCSVåˆ° raw_csv_dataã€‚
    é€»è¾‘ï¼šè¿›å…¥ç»Ÿè®¡æ•°æ® â†’ å–æœ€æ–°å¹´ä»½ â†’ æšä¸¾è¯¥å¹´ä»½ä¸‹æ‰€æœ‰å‡ºç°çš„æœˆä»½ï¼ˆæœ€åä¸€ä¸ªå³æœ€æ–°ï¼‰â†’ è¿›å…¥æœˆä»½é¡µé¦–æ¡æ–‡ç«  â†’ ä¸‹è½½ .xlsx â†’ è¯»å–â€œåä¸€åœ°å¸‚â€è¡¨å¹¶ä¿å­˜ã€‚
    å·²ä¸‹è½½çš„â€œæµ™æ±Ÿçœ-YYYY-MM.csvâ€å°†è‡ªåŠ¨è·³è¿‡ã€‚
    """

    month_mapping = {
        "ä¸€æœˆ": 1, "äºŒæœˆ": 2, "ä¸‰æœˆ": 3, "å››æœˆ": 4, "äº”æœˆ": 5, "å…­æœˆ": 6,
        "ä¸ƒæœˆ": 7, "å…«æœˆ": 8, "ä¹æœˆ": 9, "åæœˆ": 10, "åä¸€æœˆ": 11, "åäºŒæœˆ": 12
    }

    def abs_url(base: str, href: str) -> str:
        href = href or ""
        return href if href.startswith("http") else f"{base}{href}"

    base = "http://hangzhou.customs.gov.cn"
    # å…¥å£é¡µï¼šä»»æ„ä¸€ä¸ªå¹´ä»½é¡µé¢å³å¯å±•ç¤ºå·¦ä¾§â€œç»Ÿè®¡æ•°æ®â€å¹´ä»½å¯¼èˆªï¼›ä½¿ç”¨å½“å‰å¯ç”¨çš„ 2025 ä¸€æœˆé¡µ
    entry_url = f"{base}/hangzhou_customs/575609/zlbd/575612/575612/6430241/6430315/index.html"

    print("\nğŸš€ å¼€å§‹ä¸‹è½½æµ™æ±Ÿçœæ•°æ®ï¼ˆè‡ªåŠ¨è¯†åˆ«æœ€æ–°æœˆä»½ï¼‰...")
    ensure_raw_data_dir()

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=HEADLESS,
            slow_mo=SLOW_MO_MS,
            args=[
                "--no-sandbox",
                "--disable-dev-shm-usage",
                "--disable-blink-features=AutomationControlled",
            ],
        )
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            viewport={"width": 1920, "height": 1080},
            locale="zh-CN",
        )
        page = context.new_page()

        try:
            # è¿›å…¥å…¥å£é¡µ
            for attempt in range(1, NAV_MAX_RETRIES + 1):
                try:
                    page.goto(entry_url, timeout=NAV_TIMEOUT_MS)
                    page.wait_for_load_state("domcontentloaded")
                    # ç­‰å¾…å·¦ä¾§â€œç»Ÿè®¡æ•°æ®â€å¯¼èˆªå‡ºç°
                    page.get_by_role("navigation", name="ç»Ÿè®¡æ•°æ®").get_by_role("link").first.wait_for(state="visible", timeout=SELECTOR_TIMEOUT_MS)
                    break
                except Exception as nav_err:
                    if attempt == NAV_MAX_RETRIES:
                        raise nav_err
                    print(f"    å¯¼èˆªå¤±è´¥ï¼Œé‡è¯• {attempt}/{NAV_MAX_RETRIES - 1} ...")
                    time.sleep(NAV_RETRY_DELAY_SEC)
            print("    å·²æ‰“å¼€ç»Ÿè®¡æ•°æ®æ ç›®é¡µé¢")

            # ç»Ÿè®¡æ•°æ® â†’ æœ€æ–°å¹´ä»½ï¼ˆç¬¬ä¸€ä¸ªé“¾æ¥ï¼‰
            years_locator = page.get_by_role("navigation", name="ç»Ÿè®¡æ•°æ®").get_by_role("link")
            latest_year_link = years_locator.first
            latest_year_text = latest_year_link.inner_text().strip()
            m = re.search(r"(\d{4})å¹´", latest_year_text)
            if not m:
                raise RuntimeError("æœªèƒ½è¯†åˆ«åˆ°å¹´ä»½æ–‡æœ¬")
            latest_year = int(m.group(1))

            latest_year_href = latest_year_link.get_attribute("href")
            latest_year_url = abs_url(base, latest_year_href)
            print(f"    æœ€æ–°å¹´ä»½ï¼š{latest_year} -> {latest_year_url}")

            # ç›´æ¥å¯¼èˆªåˆ°è¯¥å¹´ä»½é¡µé¢ï¼ˆè€Œéç‚¹å‡»ï¼Œé¿å…æ–°å¼€é¡µ/åŒé¡µä¸ä¸€è‡´é—®é¢˜ï¼‰
            page.goto(latest_year_url, timeout=NAV_TIMEOUT_MS)
            page.wait_for_load_state("domcontentloaded")
            page.wait_for_timeout(1500)

            # åœ¨â€œYYYYå¹´â€å¯¼èˆªä¸­è¯»å–æ‰€æœ‰æœˆä»½é“¾æ¥ï¼ˆå·²å‘å¸ƒçš„æœˆä»½æ‰ä¼šå‡ºç°ï¼‰
            months_locator = page.get_by_role("navigation", name=f"{latest_year}å¹´").get_by_role("link")
            month_count = months_locator.count()
            if month_count == 0:
                raise RuntimeError("æœªæ‰¾åˆ°æœˆä»½å¯¼èˆªé“¾æ¥")

            months = []  # [(month_num, month_name_cn, month_url)]
            for i in range(month_count):
                link = months_locator.nth(i)
                name = link.inner_text().strip()
                if name not in month_mapping:
                    continue
                href = link.get_attribute("href") or ""
                url = abs_url(base, href)
                months.append((month_mapping[name], name, url))

            # æŒ‰æœˆä»½å‡åºï¼Œä¾¿äºè¡¥é½ç¼ºå¤±æœˆä»½ï¼›æœ€åä¸€ä¸ªå³â€œæœ€æ–°â€
            months.sort(key=lambda x: x[0])
            if not months:
                raise RuntimeError("æœˆä»½è§£æä¸ºç©º")

            print("    å‘ç°æœˆä»½ï¼š" + ", ".join([f"{mcn}({mnum:02d})" for mnum, mcn, _ in months]))

            for month_num, month_cn, month_url in months:
                csv_name = f"æµ™æ±Ÿçœ-{latest_year}-{month_num:02d}.csv"
                csv_path = os.path.join(RAW_DATA_PATH, csv_name)
                if os.path.exists(csv_path):
                    print(f"âœ… {latest_year}å¹´{month_num}æœˆå·²å­˜åœ¨ï¼Œè·³è¿‡")
                    continue

                print(f"ğŸ“¥ å¤„ç† {latest_year}å¹´{month_num}æœˆ â†’ {month_url}")
                try:
                    page.goto(month_url, timeout=NAV_TIMEOUT_MS)
                    page.wait_for_load_state("domcontentloaded")
                    page.wait_for_timeout(1500)

                    # æœˆä»½é¡µé¦–æ¡â€œè¿›å‡ºå£æƒ…å†µâ€æ–‡ç« é“¾æ¥
                    article_link = None
                    # ä¼˜å…ˆæ›´ç²¾ç¡®
                    candidates = page.locator("a:has-text('æµ™æ±Ÿçœè¿›å‡ºå£æƒ…å†µ')").all()
                    if not candidates:
                        candidates = page.locator("a:has-text('è¿›å‡ºå£æƒ…å†µ')").all()
                    if not candidates:
                        candidates = page.locator("a:has-text('è¿›å‡ºå£')").all()
                    if candidates:
                        article_link = candidates[0]

                    if not article_link:
                        print(f"    âŒ {latest_year}å¹´{month_num}æœˆæœªæ‰¾åˆ°æ–‡ç« é“¾æ¥")
                        continue

                    article_href = article_link.get_attribute("href") or ""
                    article_url = abs_url(base, article_href)
                    print(f"    æ‰“å¼€æ–‡ç« ï¼š{article_url}")

                    article_page = context.new_page()
                    article_page.goto(article_url, timeout=NAV_TIMEOUT_MS)
                    article_page.wait_for_load_state("domcontentloaded")
                    article_page.wait_for_timeout(1500)

                    # æŸ¥æ‰¾ Excel ä¸‹è½½é“¾æ¥ï¼Œä¼˜å…ˆ .xlsx
                    def find_excel_links() -> list:
                        # æ’é™¤åˆ†äº«/ç¤¾äº¤é“¾æ¥
                        links = article_page.locator('a[href$=".xlsx"], a[href$=".xls"]').all()
                        cleaned = []
                        for lk in links:
                            href = (lk.get_attribute("href") or "").lower()
                            txt = (lk.inner_text() or "").lower()
                            if any(x in href for x in ["share", "qzone", "weibo", "wechat"]) or \
                               any(x in txt for x in ["åˆ†äº«", "share", "è½¬å‘"]):
                                continue
                            cleaned.append(lk)
                        return cleaned

                    excel_links = find_excel_links()
                    if not excel_links:
                        print("    âŒ æœªå‘ç°Excelä¸‹è½½é“¾æ¥")
                        article_page.close()
                        continue

                    # å–ç¬¬ä¸€ä¸ª Excel é“¾æ¥
                    excel_link = excel_links[0]
                    link_text = excel_link.inner_text() or "(æ— æ ‡é¢˜)"
                    link_href = excel_link.get_attribute("href") or ""
                    print(f"    ï¿½ å‡†å¤‡ä¸‹è½½ï¼š{link_text} -> {link_href}")

                    with article_page.expect_download() as dl_info:
                        excel_link.click()
                    dl = dl_info.value

                    temp_excel = f"temp_{latest_year}_{month_num}.xlsx"
                    dl.save_as(temp_excel)

                    try:
                        df = pd.read_excel(temp_excel, sheet_name="åä¸€åœ°å¸‚")
                        if not df.empty:
                            df_clean = df.dropna(subset=["é¡¹ç›®"]).rename(columns={"é¡¹ç›®": "æ”¶å‘è´§äººæ‰€åœ¨åœ°"})
                            # å•ä½è½¬æ¢è§„åˆ™ä¿æŒä¸åŸé€»è¾‘ä¸€è‡´
                            if (latest_year == 2024 and month_num >= 7) or latest_year >= 2025:
                                for col in ["å½“æœŸè¿›å‡ºå£", "å½“æœŸè¿›å£", "å½“æœŸå‡ºå£"]:
                                    if col in df_clean.columns:
                                        df_clean[col] = df_clean[col] * 10000
                            save_raw_df(df_clean, csv_path)
                            print(f"âœ… å·²ä¿å­˜ï¼š{csv_path}")
                    except Exception as e:
                        print(f"    âŒ æ•°æ®å¤„ç†å¤±è´¥ï¼š{e}")
                    finally:
                        if os.path.exists(temp_excel):
                            os.remove(temp_excel)
                        article_page.close()

                    time.sleep(random.uniform(2.0, 4.0))
                except Exception as e:
                    print(f"âŒ {latest_year}å¹´{month_num}æœˆä¸‹è½½å¤±è´¥ï¼š{e}")
                    continue

            print("âœ… æµ™æ±Ÿçœæ•°æ®ä¸‹è½½å®Œæˆ")
        finally:
            browser.close()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æµ·å…³æ•°æ®ä¸‹è½½å™¨")
    print("=" * 50)
    print("ğŸ’¡ æç¤ºï¼šè„šæœ¬ä¼šè‡ªåŠ¨è·³è¿‡å·²ä¸‹è½½çš„æ–‡ä»¶")
    print("é€‰æ‹©ä¸‹è½½é€‰é¡¹:")
    print("1. ä¸‹è½½å…¨å›½æ•°æ®")
    print("2. ä¸‹è½½æµ™æ±Ÿçœæ•°æ®") 
    print("3. ä¸‹è½½æ‰€æœ‰æ•°æ®")
    print("0. é€€å‡º")
    
    try:
        choice = input("\nè¯·é€‰æ‹© (0-3ï¼Œé»˜è®¤3): ").strip()
        
        if choice == "1":
            scrape_raw_data_to_csv()
        elif choice == "2":
            download_zhejiang_data()
        elif choice == "3" or choice == "":
            scrape_raw_data_to_csv()
            download_zhejiang_data()
        elif choice == "0":
            print("ğŸ‘‹ é€€å‡º")
            return
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œé”™è¯¯: {e}")


if __name__ == "__main__":
    main()
